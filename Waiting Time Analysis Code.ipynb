{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "coilData =pd.read_csv('CoilHistory.csv')\n",
    "operData =pd.read_csv('OperationsDetail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select useful attributes with dictionary\n",
    "coilDict = [0,2,3,4,5,6,8,9,11,14,15,16,20,21,31,32,33,34,36,37,38,39,40,41,44,45,46,48,49,50,51,52,53,54,56,57,58,59,\n",
    "           61,62,63,64,65,66,68,69,70,71,73,74,75,76,77,78,79,80]\n",
    "operDict  = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,25]\n",
    "coil = coilData.iloc[:,coilDict]\n",
    "oper = operData.iloc[:,operDict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1: Missing Value Analysis\n",
    "missing =coil.replace(\" \",np.nan)\n",
    "missing =missing.replace(\"  \",np.nan)\n",
    "missing =missing.replace(\"   \",np.nan)\n",
    "missing =missing.replace(\"    \",np.nan)\n",
    "missing =missing.replace(\"     \",np.nan)\n",
    "missing =missing.replace(\"      \",np.nan)\n",
    "missing =missing.replace(\"       \",np.nan)\n",
    "missing =missing.replace(\"        \",np.nan)\n",
    "\n",
    "total_missing = missing.isnull().sum().sort_values(ascending = False)\n",
    "percent_missing = (missing.isnull().sum()/missing.isnull().count()*100).sort_values(ascending = False)\n",
    "missing4  = pd.concat([total_missing, percent_missing], axis=1, keys=['Total_Missing', 'Percent_Missing(%)'])\n",
    "print(\"\\n-----------Missing Value is--------------\\n\",missing4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the attributes which have more than 50% missing value\n",
    "coil_1 = coil.drop(['INUCLS','INUNIT','INFNXO','INFNXU','INMAJL','INMINL','INSTRN','INTRST'], axis=1)\n",
    "coil_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Data Type\n",
    "cols = coil_1.columns.tolist()\n",
    "#Describe the variables\n",
    "cat_vars = []\n",
    "print (\"\\n the catagorical attributes are:\")\n",
    "for col in cols:\n",
    "    if coil_1[col].dtype == \"object\":\n",
    "        print (col)\n",
    "        cat_vars.append(col)\n",
    "\n",
    "#Use LableEncoder to transform data type\n",
    "print (\"\\n Start transform attributes...\")       \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in cat_vars:\n",
    "    tran = le.fit_transform(coil_1[col].tolist())\n",
    "    tran_df = pd.DataFrame(tran, columns=['num_'+col])\n",
    "    print(\"{col} is transformed into {num_col}\".format(col=col,num_col='num_'+col))\n",
    "#    print (le.classes_)\n",
    "    coil_1 = pd.concat([coil_1, tran_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust for some attributes which should NOT be categorical variables\n",
    "coil_2 = coil_1.drop(['num_INSTMS','num_INFTMS'], axis=1)\n",
    "#coil_2[['INELAP']] = coil_2[['INELAP']].apply(pd.to_numeric)\n",
    "\n",
    "#Add New Attribute\n",
    "coil_2['yield loss']=(coil_2['INFGWT']-coil_2['INSLBS'])/coil_2['INSLBS']\n",
    "coil_2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#Types of Features\n",
    "#Type of Machine\n",
    "#1.Distribution of Some Selected Attributes\n",
    "coil_2.loc[:,'INSNXO'].value_counts()\n",
    "coil_2.loc[:,'INSRSQ'].value_counts()\n",
    "coil_2.loc[:,'INSENT'].value_counts()\n",
    "coil_2.loc[:,'INALLY'].value_counts()\n",
    "coil_2.loc[:,'INDENS'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coil_2.loc[:,'INHEAT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2,2,figsize=(20,15))\n",
    "sns.countplot('INSWID',data=coil_2,ax=ax[0,0])\n",
    "ax[0,0].set_title('No. of Steel Type')\n",
    "sns.countplot('INSNXO',data=coil_2,ax=ax[0,1])\n",
    "ax[0,1].set_title('No. of Operation Type')\n",
    "sns.countplot('INALLY',data=coil_2,ax=ax[1,0])\n",
    "ax[1,0].set_title('No. of Alloy Type')\n",
    "sns.countplot('INSNXU',data=coil_2,ax=ax[1,1])\n",
    "ax[1,1].set_title('No. of Next Unit')\n",
    "plt.subplots_adjust(wspace=0.2,hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waitData=coil_2[coil_2['INSNXO']==\"IN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waitData['prev_time'] = waitData.groupby(['INVID#'])['INFTMS'].shift(1)\n",
    "waitData['prev_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in range(len(waitData)):\n",
    "        if pd.isnull(waitData[\"prev_time\"][waitData.index[i]])==False:\n",
    "           b =waitData.index[i]-waitData.index[i-1]-1\n",
    "        else:\n",
    "            b=0\n",
    "        a.append(b)\n",
    "waitData['count_steps']=a          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coil_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#waitData[['INSENT','INSNXO','count_steps']]\n",
    "coil_2[[\"INSGWT\",\"INFGWT\",\"yield loss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def interval_seconds(prev,start):\n",
    "    if pd.isnull(prev)==False:\n",
    "        prev=str(prev)\n",
    "        prev=prev[0:19]\n",
    "        start=str(start)\n",
    "        start=start[0:19]\n",
    "        prev=datetime.datetime.strptime(prev, '%Y-%m-%d %H:%M:%S')\n",
    "        start=datetime.datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
    "        i=start-prev\n",
    "        i=i.seconds\n",
    "    else:\n",
    "        i=0\n",
    "    return(i)\n",
    "\n",
    "def interval_days(prev,start):\n",
    "    if pd.isnull(prev)==False:\n",
    "        prev=str(prev)\n",
    "        prev=prev[0:19]\n",
    "        start=str(start)\n",
    "        start=start[0:19]\n",
    "        prev=datetime.datetime.strptime(prev, '%Y-%m-%d %H:%M:%S')\n",
    "        start=datetime.datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
    "        i=start-prev\n",
    "        i=i.days\n",
    "    else:\n",
    "        i=0\n",
    "    return(i)\n",
    "\n",
    "waitData['w1'] = waitData.apply(lambda x: interval_seconds(x.prev_time,x.INSTMS), axis = 1).to_frame()\n",
    "waitData['w2'] = waitData.apply(lambda x: interval_days(x.prev_time,x.INSTMS), axis = 1).to_frame()\n",
    "waitData['waitTime']=waitData['w1']+waitData['w2']*86400\n",
    "\n",
    "waitData= waitData.drop(['w1','w2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use hour as waitTime unit\n",
    "waitData['waitTime']=waitData['waitTime']/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------Step1: correlation between waiting time and attributes--------\n",
    "corrData = waitData[['waitTime','num_INTYPE','INDENS','INHEAT','IN#PAS','INALLY','num_INALLY','num_INVCLK','num_INSNXO','num_INSACT','INSWID','INSGAG','INSLEN','INSGWT','num_INFACT','num_INFSTA','INFGWT','INELAP','yield loss']]\n",
    "sns.heatmap(corrData.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Step2: the waitTime VS. attributes------\n",
    "#1. Width\n",
    "waitData['INSWID'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waitData['waitTime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,3,figsize=(20,8))\n",
    "temp=waitData['waitTime']\n",
    "sns.distplot(waitData[waitData['waitTime']<=temp.mean(axis=0)].INSWID,ax=ax[0])\n",
    "ax[0].set_title('width distribution in Short waitTime')\n",
    "\n",
    "sns.distplot(waitData[waitData['waitTime']>temp.mean(axis=0)].INSWID,ax=ax[1])\n",
    "ax[1].set_title('width distribution in Long waitTime')\n",
    "\n",
    "sns.distplot(waitData.INSWID,ax=ax[2])\n",
    "ax[2].set_title('width distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "waitData[\"waitTime_tooLong\"]= (waitData['waitTime']>=1.822699e+05)\n",
    "sns.violinplot(\"INSWID\",\"INSGWT\", hue=\"waitTime_tooLong\", data=waitData,split=True,ax=ax[0])\n",
    "ax[0].set_title('width and weight vs whether wait too long')\n",
    "ax[0].set_yticks(range(0,55,5))\n",
    "sns.violinplot(\"INSLEN\",\"INSGAG\", hue=\"waitTime_tooLong\", data=waitData,split=True,ax=ax[1])\n",
    "ax[1].set_title('length and Gauge vs whether wait too long')\n",
    "#ax[1].set_yticks(range(0,0.8,0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Gauge\n",
    "waitData['INSGAG'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,3,figsize=(20,8))\n",
    "sns.distplot(waitData[waitData['INSGAG']<=0.078261].INSGAG,ax=ax[0])\n",
    "ax[0].set_title('waitTime in Small Gauge')\n",
    "sns.distplot(waitData[waitData['INSGAG']>0.078261].INSGAG,ax=ax[1])\n",
    "ax[1].set_title('waitTime in Large Gauge')\n",
    "sns.distplot(waitData.INSGAG,ax=ax[2])\n",
    "ax[2].set_title('waitTime in All Gauge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------Step3:Feature Engineering------------\n",
    "##---------Transform Categorical Variable--------###\n",
    "pick_x_cat =[4,7,12,13,14,15,24,29,36,42]\n",
    "pick_x=[2,6,10,16,17,18,21,22,25,26,30,33,34,35,39,40,43,44,47,71,73]+pick_x_cat\n",
    "pick_y=[74]                                                  \n",
    "\n",
    "waitData=waitData.dropna() \n",
    "x = waitData.iloc[:,pick_x]\n",
    "y = waitData.iloc[:,pick_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Sankey Diagram\n",
    "x.to_csv(\"x_sankey.csv\",index=False,sep=',')\n",
    "y.to_csv(\"y_sankey.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------Method1:Target Encoding-----------\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Target encoder.\n",
    "    \n",
    "    Replaces categorical column(s) with the mean target value for\n",
    "    each category.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cols=None):\n",
    "        \"\"\"Target encoder\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cols : list of str\n",
    "            Columns to target encode.  Default is to target \n",
    "            encode all categorical columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        if isinstance(cols, str):\n",
    "            self.cols = [cols]\n",
    "        else:\n",
    "            self.cols = cols\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit target encoder to X and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode all categorical cols by default\n",
    "        if self.cols is None:\n",
    "            self.cols = [col for col in X \n",
    "                         if str(X[col].dtype)=='object']\n",
    "\n",
    "        # Check columns are in X\n",
    "        for col in self.cols:\n",
    "            if col not in X:\n",
    "                raise ValueError('Column \\''+col+'\\' not in X')\n",
    "\n",
    "        # Encode each element of each column\n",
    "        self.maps = dict() #dict to store map for each column\n",
    "        for col in self.cols:\n",
    "            tmap = dict()\n",
    "            uniques = X[col].unique()\n",
    "            for unique in uniques:\n",
    "                tmap[unique] = y[X[col]==unique].mean()\n",
    "            self.maps[col] = tmap\n",
    "            \n",
    "        return self\n",
    "\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Perform the target encoding transformation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        Xo = X.copy()\n",
    "        for col, tmap in self.maps.items():\n",
    "            vals = np.full(X.shape[0], np.nan)\n",
    "            for val, mean_target in tmap.items():\n",
    "                vals[X[col]==val] = mean_target\n",
    "            Xo[col] = vals\n",
    "        return Xo\n",
    "            \n",
    "            \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform the data via target encoding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values (required!).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encode the categorical data\n",
    "te = TargetEncoder()\n",
    "x_target_encoded = te.fit_transform(x, y)\n",
    "x_target_encoded.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#according to attribute meaning --> INALLOY divided according to the first digit;\n",
    "#x_target_encoded[\"INALLY\"]=str(x[\"INALLY\"])[0]\n",
    "x_target_encoded['INALLY']=x['INALLY'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target_encoded[\"INALLY\"]=x_target_encoded[\"INALLY\"].replace(\"U\",10)\n",
    "x_target_encoded[\"INALLY\"]=x_target_encoded[\"INALLY\"].replace(\"K\",11)\n",
    "x_target_encoded[\"INALLY\"]=x_target_encoded[\"INALLY\"].replace(\"C\",12)\n",
    "x_target_encoded[\"INALLY\"]=x_target_encoded[\"INALLY\"].replace(\"T\",13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "#Embeded Method:\n",
    "reg = LassoCV()\n",
    "reg.fit(x_target_encoded, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(x_target_encoded, y))\n",
    "coef = pd.Series(reg.coef_, index = x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrData = pd.concat( [x_target_encoded,y], axis=1 )\n",
    "corrData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter Method\n",
    "#Using Pearson Correlation\n",
    "corrData = pd.concat( [x_target_encoded,y], axis=1 )\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = corrData.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"waitTime\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.4]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Wrapper Method\n",
    "#Feed the features to the selected Machine Learning algorithm and based on the \n",
    "#model performance you add/remove the features.\n",
    "#i. Backward Elimination\n",
    "#Adding constant column of ones, mandatory for sm.OLS model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Backward Elimination\n",
    "cols = list(x_target_encoded.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    x_new = x_target_encoded[cols]\n",
    "    #x_new = sm.add_constant(x_new)\n",
    "    model = sm.OLS(y,x_new.astype(float)).fit()\n",
    "    p = pd.Series(model.pvalues.values[0:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "selected_features_BE = cols\n",
    "print(selected_features_BE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE (Recursive Feature Elimination)\n",
    "#no of features\n",
    "nof_list=np.arange(1,13)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_target_encoded,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfe = RFE(model, 5)\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(x_target_encoded,y)  \n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_target_encoded.columns #'INDENS', 'INSGAG',INFGAG','INSPCS', 'INFPCS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conlusion: we will select:\n",
    "#INDENS INSGAG INSPCS INFAG INFPCS\n",
    "#INSXNU INFACT INHEAT\n",
    "#INSSCL IN#PAS\n",
    "#INALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_sel =[0,1,4,9,10,12,17,20,21,22,23,26]\n",
    "x_selected = x_target_encoded.iloc[:,x_sel]\n",
    "x_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save files for further use\n",
    "x_selected.to_csv(\"x_selected.csv\",index=False,sep=',')\n",
    "y.to_csv(\"y.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x_selected =pd.read_csv('x_selected.csv')\n",
    "y =pd.read_csv('y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####----------Build ML Model----------###########\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#------<Model1: LightGBM>----------\n",
    "data_prepared = x_selected.astype(float)\n",
    "data_labels = y.astype(int)\n",
    "\n",
    "# Split into training and testing data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(data_prepared, data_labels,random_state = 42)\n",
    "get_ipython().system('pip install lightgbm')\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Evaluation of the Model\n",
    "# Visualization\n",
    "plt.rcParams['font.size'] = 18\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# Governing choices for search\n",
    "N_FOLDS = 5\n",
    "MAX_EVALS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we can create a model with the default value of hyperparameters and score it using cross validation with early stopping. Using the cv LightGBM function requires creating a Dataset.\n",
    "# Note: here we use **Automated Hyperparameter Tuning**\n",
    "#Establish a baseline model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "base_LGBM = lgb.LGBMRegressor(random_state=50)\n",
    "# Training set\n",
    "train_set = lgb.Dataset(train_features, label = train_labels)\n",
    "test_set = lgb.Dataset(test_features, label = test_labels)\n",
    "\n",
    "# Default hyperparamters\n",
    "hyperparameters = base_LGBM.get_params()\n",
    "baseline_error = cross_val_score(base_LGBM, train_features, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "baseline_MSE_mean= baseline_error.mean()\n",
    "print('The MSE for LGBM is:', baseline_MSE_mean)\n",
    "\n",
    "\n",
    "# Train and make predicions with model\n",
    "base_LGBM.fit(train_features, train_labels)\n",
    "y_pred = base_LGBM.predict(test_features)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_Test=mean_squared_error(test_labels, y_pred)\n",
    "print('The baseline model scores {:.5f} RSE on the test set.'.format(MSE_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the parameter\n",
    "#----Hyper Parameter Tuning: Use GridSearchCV--------------\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "import math\n",
    "def lightGBM_model(X, y):\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=lgb.LGBMRegressor(random_state=50),\n",
    "        param_grid={\n",
    "            'learning_rate': [0.01],\n",
    "            'num_leaves': [24,50,80],\n",
    "            'max_depth': [-1],\n",
    "            'max_bin':[20,90],\n",
    "            'min_child_samples': [20,30],\n",
    "            'min_child_weight':[0.001],\n",
    "            'n_estimators':[100,200],\n",
    "            'subsample': [0.01, 1.0] ,\n",
    "            'subsample_for_bin':[100000,200000],\n",
    "            'reg_alpha':[0.0],\n",
    "            'reg_lambda':[0.0]\n",
    "        },\n",
    "        cv=10, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "\n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    best_lightGBM = lgb.LGBMRegressor(num_leaves=best_params[\"num_leaves\"], max_bin=best_params[\"max_bin\"], \n",
    "                                      min_child_samples=best_params[\"min_child_samples\"],\n",
    "                                     n_estimators=best_params[\"n_estimators\"],\n",
    "                                      subsample=best_params[\"subsample\"],\n",
    "                                      subsample_for_bin=best_params[\"subsample_for_bin\"])\n",
    "\n",
    "    scoring = {\n",
    "               'abs_error': 'neg_mean_absolute_error',\n",
    "               'squared_error': 'neg_mean_squared_error',\n",
    "                \"r2\":\"r2\"                               }\n",
    "\n",
    "    scores = cross_validate(best_lightGBM, X, y, cv=10, scoring=scoring, return_train_score=True)\n",
    "    return \"MAE :\", abs(scores['test_abs_error'].mean()), \"| RMSE :\", math.sqrt(abs(scores['test_squared_error'].mean())),\"| r2 :\", scores['test_r2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X=data_prepared\n",
    "y=data_labels\n",
    "lightGBM_model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mode2: Random Forest\n",
    "#---------Divide training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#or use pipeline function to get the same purpose.\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                        RandomForestRegressor(n_estimators=100))\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    "#------cross validation---------\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    "y_train_new=y_train/3600\n",
    "y_test_new=y_test/3600\n",
    "clf.fit(X_train, y_train_new)\n",
    "y_pred_new = clf.predict(X_test)\n",
    "#-------cv=10------------\n",
    "print(mean_squared_error(y_test_new, y_pred_new))\n",
    "print(r2_score(y_test_new, y_pred_new))\n",
    "print(mean_squared_error(y_test_new, y_pred_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Model3.Using Neural Network Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_test_new=y_test/3600\n",
    "y_train_new=y_train/3600\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(64,activation='relu',input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy'])\n",
    "model.fit(X_train_scaled,y_train_new,batch_size=32,epochs=10,verbose=1,validation_data=(X_test_scaled,y_test_new))\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy'])\n",
    "model.fit(X_train_scaled,y_train_new,batch_size=32,epochs=10,verbose=1,validation_data=(X_test_scaled,y_test_new))\n",
    "print(mean_squared_error(y_test_new, y_pred_new))\n",
    "print (clf.best_params_)\n",
    "print(r2_score(y_test_new, y_pred_new))\n",
    "print(mean_squared_error(y_test_new, y_pred_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Model4.Using KNeighborsRegressor-----------\n",
    "#n_neighbors: the number of neighbors, is set to 5\n",
    "#algorithm: for computing nearest neighbors, is set to auto\n",
    "#p: set to 2, corresponding to Euclidean distance\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "hyperparameters = {'n_neighbors' : [7,8,9,10], 'algorithm' : ['auto', 'brute', 'ball_tree','kd_tree'],\n",
    "                  'weights': ['uniform','distance']}\n",
    "\n",
    "clf = GridSearchCV(knn, hyperparameters)\n",
    "y_train_new=y_train/3600\n",
    "y_test_new=y_test/3600\n",
    "clf.fit(X_train, y_train_new)\n",
    "y_pred_new = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best KNN\n",
    "print (clf.best_params_)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "import math\n",
    "\n",
    "y_new=y/3600\n",
    "\n",
    "best_knn = KNeighborsRegressor(algorithm='brute', n_neighbors=9, weights='uniform')\n",
    "scores = cross_validate(best_knn, X, y_new, cv=10, scoring={'abs_error': 'neg_mean_absolute_error','squared_error': 'neg_mean_squared_error',\"r2\":\"r2\"}, return_train_score=True)\n",
    "# evaluations\n",
    "print(abs(scores['test_abs_error'].mean()))\n",
    "print(math.sqrt(abs(scores['test_squared_error'].mean())))\n",
    "print(scores['test_r2'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Model5: Linear Regression--------------\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "y_test_new=y_test/3600\n",
    "y_train_new=y_train/3600\n",
    "regr.fit(X_train, y_train_new)\n",
    "LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)\n",
    "print(regr.coef_) \n",
    "print(regr.intercept_)\n",
    "y_pred_new = regr.predict(X_test)\n",
    "print(y_pred_new)\n",
    "print(mean_absolute_error(y_test_new,y_pred_new))\n",
    "print(r2_score(y_test_new, y_pred_new))\n",
    "print(mean_squared_error(y_test_new, y_pred_new))\n",
    "print(math.sqrt(mean_squared_error(y_test_new, y_pred_new)))\n",
    "hyperparameters = regr.get_params()\n",
    "hyperparameters\n",
    "y_pred_new = regr.predict(X_test_std)\n",
    "print(y_pred_new)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_test_new,y_pred_new))\n",
    "print(r2_score(y_test_new, y_pred_new))\n",
    "print(math.sqrt(mean_squared_error(y_test_new, y_pred_new)))#------ no significant difference\n",
    "regr2 = linear_model.LinearRegression(copy_X=True, fit_intercept=True, n_jobs=2, normalize=True)\n",
    "regr2.fit(X_train_std, y_train_new)\n",
    "print(regr2.coef_) \n",
    "y_pred_new = regr2.predict(X_test_std)\n",
    "print(y_pred_new)\n",
    "print(r2_score(y_test_new, y_pred_new))\n",
    "print(mean_squared_error(y_test_new, y_pred_new))\n",
    "print(math.sqrt(mean_squared_error(y_test_new, y_pred_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------Model6: Bayesianridge-----------\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "reg = BayesianRidge()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "reg.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = reg.predict(X_test)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "reg_br = BayesianRidge()\n",
    "hyperparameters = {'alpha_1' : [1e-06,2e-06,3e-03,4e-01], 'alpha_2' : [1e-06, 0.1, 1,10],\n",
    "                  'lambda_1': [1e-06,2e-06,3e-03,4e-01],'lambda_2':[1e-06, 0.1, 1,10]}\n",
    "\n",
    "clf = GridSearchCV(reg_br, hyperparameters, cv=10, scoring='neg_mean_squared_error')\n",
    "clf.fit(X, y_new)\n",
    "print (clf.best_params_)#best bayesianridge\n",
    "\n",
    "best_br= BayesianRidge(alpha_1=1e-06, alpha_2= 10, lambda_1= 0.4, lambda_2= 1e-06)\n",
    "best_br.fit(X_train, y_train_new)\n",
    "y_pred_new = best_br.predict(X_test)\n",
    "\n",
    "\n",
    "print(mean_absolute_error(y_test_new,y_pred_new))\n",
    "print(r2_score(y_test_new, y_pred_new))\n",
    "print(math.sqrt(mean_squared_error(y_test_new, y_pred_new)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
